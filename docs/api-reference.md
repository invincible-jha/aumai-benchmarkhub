# API Reference — aumai-benchmarkhub

Full reference for every public class, function, and model in `aumai-benchmarkhub 0.1.0`.

---

## Module: `aumai_benchmarkhub.models`

### `BenchmarkCategory`

```python
class BenchmarkCategory(str, Enum)
```

High-level capability categories for benchmark cases. Inherits from `str` so members
can be used directly in JSON serialisation and CLI argument parsing.

| Member | Value | Description |
|---|---|---|
| `reasoning` | `"reasoning"` | Logical inference, deduction, common sense |
| `tool_use` | `"tool_use"` | Calling functions, parsing structured output, using APIs |
| `planning` | `"planning"` | Multi-step goal decomposition and sequencing |
| `coding` | `"coding"` | Writing, debugging, or explaining code |
| `safety` | `"safety"` | Refusing harmful requests; following safety guidelines |
| `robustness` | `"robustness"` | Handling adversarial inputs, edge cases, noise |

---

### `Difficulty`

```python
Difficulty = Literal["easy", "medium", "hard"]
```

A type alias for the three valid difficulty strings. Used in `BenchmarkCase.difficulty`.

Difficulty affects the weight of a case in the overall score:

| Value | Weight |
|---|---|
| `"easy"` | 1.0 |
| `"medium"` | 1.5 |
| `"hard"` | 2.0 |

---

### `BenchmarkCase`

```python
class BenchmarkCase(BaseModel)
```

A single benchmark evaluation case. Each case specifies a prompt, the expected
behavioral properties of a correct response, and metadata.

**Fields:**

| Field | Type | Default | Description |
|---|---|---|---|
| `case_id` | `str` | — | Unique identifier for the case (required) |
| `category` | `BenchmarkCategory` | — | Capability category (required) |
| `prompt` | `str` | — | The input prompt to send to the agent (required) |
| `expected_behavior` | `dict[str, object]` | — | Evaluation criteria dict (required; see check types below) |
| `difficulty` | `Difficulty` | `"medium"` | Difficulty level affecting score weight |
| `tags` | `list[str]` | `[]` | Arbitrary tags for filtering and organisation |

**`expected_behavior` check keys:**

| Key | Value Type | Description |
|---|---|---|
| `contains` | `list[str]` | All strings must appear in the output (case-insensitive) |
| `not_contains` | `list[str]` | None of these strings may appear in the output |
| `regex` | `str` | Output must match this regex (ReDoS-safe validation applied) |
| `min_length` | `int` | Output must have at least this many characters |
| `max_length` | `int` | Output must have no more than this many characters |
| `json_valid` | `bool` | If `True`, output must be parseable as valid JSON |

Multiple keys can be combined in one `expected_behavior` dict. All defined checks must
pass for the case to be marked `passed = True`.

**Example:**

```python
from aumai_benchmarkhub.models import BenchmarkCase, BenchmarkCategory

case = BenchmarkCase(
    case_id="demo_001",
    category=BenchmarkCategory.coding,
    prompt="Write a Python function that reverses a string.",
    expected_behavior={
        "contains":   ["def ", "return"],
        "min_length": 30,
        "regex":      r"def \w+\(.*\):",
    },
    difficulty="medium",
    tags=["python", "string-manipulation"],
)

# Serialise
print(case.model_dump_json(indent=2))
```

---

### `BenchmarkSuite`

```python
class BenchmarkSuite(BaseModel)
```

A named, versioned collection of `BenchmarkCase` objects.

**Fields:**

| Field | Type | Default | Description |
|---|---|---|---|
| `suite_id` | `str` | — | Unique UUID (auto-generated by `SuiteBuilder.create_suite`) |
| `name` | `str` | — | Human-readable suite name |
| `version` | `str` | `"1.0.0"` | Semantic version string |
| `cases` | `list[BenchmarkCase]` | `[]` | List of benchmark cases |

**Example:**

```python
from aumai_benchmarkhub.models import BenchmarkSuite, BenchmarkCase, BenchmarkCategory

suite = BenchmarkSuite(
    suite_id="550e8400-e29b-41d4-a716-446655440000",
    name="My Suite",
    version="2.0.0",
    cases=[
        BenchmarkCase(
            case_id="s_001",
            category=BenchmarkCategory.reasoning,
            prompt="Is the sky blue?",
            expected_behavior={"contains": ["yes"]},
            difficulty="easy",
        )
    ],
)

# Serialise to JSON file
Path("suite.json").write_text(suite.model_dump_json(indent=2))
```

---

### `BenchmarkScore`

```python
class BenchmarkScore(BaseModel)
```

The evaluation result for a single benchmark case.

**Fields:**

| Field | Type | Default | Constraint | Description |
|---|---|---|---|---|
| `case_id` | `str` | — | — | ID of the case that was evaluated |
| `passed` | `bool` | — | — | `True` if all checks passed (`score == 1.0`) |
| `score` | `float` | — | `[0.0, 1.0]` | `checks_passed / checks_total` |
| `details` | `dict[str, object]` | `{}` | — | Failure details (missing tokens, regex errors, etc.) |
| `latency_ms` | `float` | `0.0` | `>= 0.0` | Evaluation latency in milliseconds |

**`details` keys populated on failure:**

| Key | Populated by | Description |
|---|---|---|
| `missing_tokens` | `contains` check | List of substrings not found in the output |
| `forbidden_found` | `not_contains` check | List of forbidden substrings that were found |
| `regex_failed` | `regex` check | The pattern that did not match |
| `regex_error` | `regex` check | Error message if the pattern was unsafe or invalid |
| `too_short` | `min_length` check | Actual output length when below minimum |
| `too_long` | `max_length` check | Actual output length when above maximum |
| `json_error` | `json_valid` check | `json.JSONDecodeError` message |

**Example:**

```python
score = runner.run_case(case, agent_output="def foo(): pass")
if not score.passed:
    print(f"Failed with score {score.score}")
    print(f"Details: {score.details}")
```

---

### `BenchmarkReport`

```python
class BenchmarkReport(BaseModel)
```

The complete report for a full benchmark suite run.

**Fields:**

| Field | Type | Constraint | Description |
|---|---|---|---|
| `suite` | `BenchmarkSuite` | — | The suite that was evaluated |
| `scores` | `list[BenchmarkScore]` | — | One score per case, in suite order |
| `overall_score` | `float` | `[0.0, 1.0]` | Difficulty-weighted aggregate score across all cases |
| `by_category` | `dict[str, float]` | — | Difficulty-weighted score per `BenchmarkCategory` value |

**Example:**

```python
import json

report = runner.run_suite(suite, agent_outputs)
print(json.dumps(report.model_dump(), indent=2))

# Access individual case results
for score in report.scores:
    status = "PASS" if score.passed else "FAIL"
    print(f"[{status}] {score.case_id}  score={score.score:.2f}")
```

---

## Module: `aumai_benchmarkhub.core`

### `UnsafePatternError`

```python
class UnsafePatternError(ValueError)
```

Raised when a user-supplied regex pattern is considered unsafe or syntactically invalid.

Conditions that trigger this error:
- Pattern exceeds 500 characters.
- Pattern contains nested quantifiers (e.g. `(a+)+`, `(.+)+`) that can cause
  catastrophic backtracking (ReDoS).
- Pattern is not valid Python `re` syntax.

---

### `ScoreCalculator`

```python
class ScoreCalculator
```

Compute weighted aggregate scores from a list of `BenchmarkScore` objects.
Can be instantiated independently or injected into `BenchmarkRunner`.

#### `overall`

```python
def overall(
    self,
    scores: list[BenchmarkScore],
    cases: list[BenchmarkCase],
) -> float
```

Return the overall difficulty-weighted score in `[0, 1]`.

**Parameters:**

| Parameter | Type | Description |
|---|---|---|
| `scores` | `list[BenchmarkScore]` | Per-case scores from `BenchmarkRunner.run_case()` |
| `cases` | `list[BenchmarkCase]` | The corresponding case definitions (for difficulty lookup) |

**Returns:** `float`. Returns `0.0` if `scores` is empty.

**Formula:**
```
overall = Σ(score_i × weight_i) / Σ(weight_i)
where weight_i = difficulty_weights[case_i.difficulty]
```

**Example:**

```python
from aumai_benchmarkhub import ScoreCalculator
from aumai_benchmarkhub.models import BenchmarkScore, BenchmarkCase, BenchmarkCategory

calc = ScoreCalculator()
overall = calc.overall(scores, cases)
print(f"Weighted overall score: {overall:.4f}")
```

---

#### `by_category`

```python
def by_category(
    self,
    scores: list[BenchmarkScore],
    cases: list[BenchmarkCase],
) -> dict[str, float]
```

Return a dict mapping category name → difficulty-weighted score.

**Parameters:**

| Parameter | Type | Description |
|---|---|---|
| `scores` | `list[BenchmarkScore]` | Per-case scores |
| `cases` | `list[BenchmarkCase]` | Case definitions (for category and difficulty lookup) |

**Returns:** `dict[str, float]` — only categories that have at least one scored case
are included.

**Example:**

```python
by_cat = calc.by_category(scores, cases)
for category, score in sorted(by_cat.items()):
    print(f"  {category:15s} {score:.4f}")
```

---

### `BenchmarkRunner`

```python
class BenchmarkRunner
```

Load suites, evaluate individual cases, and produce full benchmark reports.

#### `__init__`

```python
def __init__(self, calculator: ScoreCalculator | None = None) -> None
```

| Parameter | Type | Default | Description |
|---|---|---|---|
| `calculator` | `ScoreCalculator \| None` | `None` | Custom score calculator; a default `ScoreCalculator()` is used if `None` |

---

#### `load_suite`

```python
def load_suite(self, path: str) -> BenchmarkSuite
```

Deserialise a `BenchmarkSuite` from a JSON file.

| Parameter | Type | Description |
|---|---|---|
| `path` | `str` | Path to the JSON file |

**Returns:** `BenchmarkSuite`.

**Raises:** `json.JSONDecodeError` if the file is not valid JSON.
`pydantic.ValidationError` if the JSON does not match the `BenchmarkSuite` schema.

**Example:**

```python
runner = BenchmarkRunner()
suite = runner.load_suite("my_suite.json")
print(f"Loaded '{suite.name}' with {len(suite.cases)} cases")
```

---

#### `run_case`

```python
def run_case(
    self,
    case: BenchmarkCase,
    agent_output: str,
) -> BenchmarkScore
```

Evaluate a single case and return a `BenchmarkScore`.

| Parameter | Type | Description |
|---|---|---|
| `case` | `BenchmarkCase` | The benchmark case definition |
| `agent_output` | `str` | The agent's response string to evaluate |

**Returns:** `BenchmarkScore` with `passed`, `score`, `details`, and `latency_ms`.

**Evaluation logic:**
1. Iterates over `expected_behavior` keys in definition order.
2. Accumulates `(checks_passed, checks_total)` from each check function.
3. If no checks are defined, falls back to `1 if agent_output.strip() else 0`.
4. `score = checks_passed / checks_total`; `passed = (score == 1.0)`.

**Example:**

```python
from aumai_benchmarkhub.models import BenchmarkCase, BenchmarkCategory

case = BenchmarkCase(
    case_id="test",
    category=BenchmarkCategory.reasoning,
    prompt="What is 2 + 2?",
    expected_behavior={"contains": ["4"], "min_length": 1},
    difficulty="easy",
)

score = runner.run_case(case, agent_output="The answer is 4.")
assert score.passed is True
assert score.score == 1.0
```

---

#### `run_suite`

```python
def run_suite(
    self,
    suite: BenchmarkSuite,
    agent_outputs: dict[str, str],
) -> BenchmarkReport
```

Evaluate all cases in `suite` using the provided `agent_outputs` mapping.

| Parameter | Type | Description |
|---|---|---|
| `suite` | `BenchmarkSuite` | The benchmark suite to evaluate |
| `agent_outputs` | `dict[str, str]` | Mapping of `case_id` → agent response string |

**Returns:** `BenchmarkReport` containing per-case scores, overall score, and
per-category scores.

**Note:** Cases whose `case_id` is not present in `agent_outputs` receive an empty
string `""` as the output. If no `expected_behavior` checks are defined, an empty
string results in a failing case.

**Example:**

```python
agent_outputs = {
    case.case_id: my_agent.respond(case.prompt)
    for case in suite.cases
}
report = runner.run_suite(suite, agent_outputs)
print(f"Overall: {report.overall_score:.4f}")
```

---

### `SuiteBuilder`

```python
class SuiteBuilder
```

Fluent builder for creating, populating, and saving `BenchmarkSuite` objects.

#### `create_suite`

```python
def create_suite(self, name: str, version: str = "1.0.0") -> BenchmarkSuite
```

Create a new empty `BenchmarkSuite` with a UUID `suite_id`.

| Parameter | Type | Default | Description |
|---|---|---|---|
| `name` | `str` | — | Human-readable suite name |
| `version` | `str` | `"1.0.0"` | Semantic version string |

**Returns:** `BenchmarkSuite` with an empty `cases` list.

---

#### `add_case`

```python
def add_case(self, suite: BenchmarkSuite, case: BenchmarkCase) -> None
```

Append `case` to `suite.cases` in-place.

| Parameter | Type | Description |
|---|---|---|
| `suite` | `BenchmarkSuite` | The suite to modify |
| `case` | `BenchmarkCase` | The case to add |

**Returns:** `None`. Modifies `suite` in-place.

---

#### `save_suite`

```python
def save_suite(self, suite: BenchmarkSuite, path: str) -> None
```

Serialise `suite` to a JSON file at `path`. Uses Pydantic's `model_dump_json(indent=2)`
for human-readable, version-control-friendly output.

| Parameter | Type | Description |
|---|---|---|
| `suite` | `BenchmarkSuite` | The suite to serialise |
| `path` | `str` | Filesystem path for the output file (will be created or overwritten) |

**Example:**

```python
from aumai_benchmarkhub import SuiteBuilder
from aumai_benchmarkhub.models import BenchmarkCase, BenchmarkCategory

builder = SuiteBuilder()
suite   = builder.create_suite("Production Suite v1", version="1.0.0")

builder.add_case(suite, BenchmarkCase(
    case_id="prod_001",
    category=BenchmarkCategory.planning,
    prompt="Break down the goal 'deploy a web app' into five steps.",
    expected_behavior={
        "min_length": 100,
        "contains": ["step"],
    },
    difficulty="medium",
))

builder.save_suite(suite, "production_suite.json")
print(f"Saved {len(suite.cases)} cases to production_suite.json")
```

---

## Module: `aumai_benchmarkhub.suites`

Three built-in `BenchmarkSuite` constants are available as module-level imports.

```python
from aumai_benchmarkhub.suites import REASONING_SUITE, TOOL_USE_SUITE, SAFETY_SUITE
```

| Constant | Description |
|---|---|
| `REASONING_SUITE` | Cases testing logical inference, deduction, and common-sense reasoning |
| `TOOL_USE_SUITE` | Cases testing structured output, JSON validity, and function-call patterns |
| `SAFETY_SUITE` | Cases testing refusal of harmful prompts and jailbreak resistance |

All three are `BenchmarkSuite` instances and can be used directly with `BenchmarkRunner.run_suite()`.

```python
from aumai_benchmarkhub import BenchmarkRunner
from aumai_benchmarkhub.suites import SAFETY_SUITE

runner = BenchmarkRunner()
report = runner.run_suite(SAFETY_SUITE, agent_outputs)
print(f"Safety score: {report.overall_score:.4f}")
```
